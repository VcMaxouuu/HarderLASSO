{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92791fdd",
   "metadata": {},
   "source": [
    "# HarderLASSO: Comprehensive Examples\n",
    "\n",
    "This notebook demonstrates the capabilities of the HarderLASSO library for neural network-based feature selection across different machine learning tasks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Regression Examples](#regression)\n",
    "3. [Classification Examples](#classification)\n",
    "4. [Survival Analysis Examples](#survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e8d22",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d32d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits, load_breast_cancer, make_regression, make_classification\n",
    "\n",
    "# HarderLASSO imports\n",
    "from HarderLASSO import HarderLASSORegressor, HarderLASSOClassifier, HarderLASSOCox\n",
    "\n",
    "# Survival analysis\n",
    "from lifelines.datasets import load_rossi, load_kidney_transplant\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d6c1e",
   "metadata": {},
   "source": [
    "## 2. Regression Examples {#regression}\n",
    "\n",
    "### 2.1 Synthetic High-Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078c673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic regression data:\n",
      "- Samples: 200\n",
      "- Features: 100\n",
      "- Informative features: 5\n",
      "\n",
      "Training set: (140, 100)\n",
      "Test set: (60, 100)\n"
     ]
    }
   ],
   "source": [
    "# Generate high-dimensional synthetic data\n",
    "n_samples, n_features = 200, 100\n",
    "n_informative = 5\n",
    "\n",
    "\n",
    "print(f\"Generating synthetic regression data:\")\n",
    "print(f\"- Samples: {n_samples}\")\n",
    "print(f\"- Features: {n_features}\")\n",
    "print(f\"- Informative features: {n_informative}\")\n",
    "\n",
    "X= np.random.normal(size=(n_samples, n_features))\n",
    "features = np.arange(n_informative)\n",
    "beta = np.random.choice([-3, -2, -1, 1, 2, 3], size=n_informative)\n",
    "y = X[:, features]@beta + np.random.normal(size=n_samples)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8536170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HarderLASSO Regressor...\n",
      "\n",
      "=== Starting No regularization phase ===\n",
      "\tAdam (both parameter groups), no regularization\n",
      "  Epoch    0: Loss = 67.166817, Bare Loss = 67.166817, Features = 100\n",
      "  Epoch   50: Loss = 2.025277, Bare Loss = 2.025277, Features = 100\n",
      "  Epoch  100: Loss = 0.064969, Bare Loss = 0.064969, Features = 100\n",
      "  Epoch  150: Loss = 0.004407, Bare Loss = 0.004407, Features = 100\n",
      "\tConverged at epoch 179 (tolerance: 1.00e-04)\n",
      "\n",
      "=== Starting Phase 1/6 ===\n",
      "\tAdam (both parameter groups), lambda=0.9316584133214098, nu=0.9\n",
      "  Epoch    0: Loss = 259.961517, Bare Loss = 0.002144, Features = 100\n",
      "  Epoch   50: Loss = 52.565552, Bare Loss = 16.386784, Features = 100\n",
      "  Epoch  100: Loss = 34.535889, Bare Loss = 16.022408, Features = 100\n",
      "  Epoch  150: Loss = 30.226700, Bare Loss = 14.148080, Features = 100\n",
      "  Epoch  200: Loss = 27.185472, Bare Loss = 13.106308, Features = 100\n",
      "  Epoch  250: Loss = 25.350626, Bare Loss = 12.373294, Features = 100\n",
      "  Epoch  300: Loss = 23.938900, Bare Loss = 12.218316, Features = 100\n",
      "  Epoch  350: Loss = 23.059191, Bare Loss = 12.098588, Features = 100\n",
      "  Epoch  400: Loss = 22.379047, Bare Loss = 11.866726, Features = 100\n",
      "  Epoch  450: Loss = 21.317877, Bare Loss = 11.833878, Features = 100\n",
      "  Epoch  500: Loss = 20.789371, Bare Loss = 11.774235, Features = 100\n",
      "  Epoch  550: Loss = 20.561945, Bare Loss = 11.748280, Features = 100\n",
      "\tConverged at epoch 551 (tolerance: 1.00e-06)\n",
      "\n",
      "=== Starting Phase 2/6 ===\n",
      "\tAdam (both parameter groups), lambda=1.7320842742919922, nu=0.7\n",
      "  Epoch    0: Loss = 28.626072, Bare Loss = 11.746600, Features = 100\n",
      "  Epoch   50: Loss = 26.190432, Bare Loss = 14.333424, Features = 100\n",
      "  Epoch  100: Loss = 25.784897, Bare Loss = 14.511548, Features = 100\n",
      "  Epoch  150: Loss = 25.485722, Bare Loss = 14.539818, Features = 100\n",
      "  Epoch  200: Loss = 25.271225, Bare Loss = 14.525370, Features = 100\n",
      "\tConverged at epoch 214 (tolerance: 1.00e-06)\n",
      "\n",
      "=== Starting Phase 3/6 ===\n",
      "\tAdam (both parameter groups), lambda=2.5325101352625747, nu=0.5\n",
      "  Epoch    0: Loss = 29.569410, Bare Loss = 14.526128, Features = 100\n",
      "  Epoch   50: Loss = 29.370380, Bare Loss = 15.428031, Features = 100\n",
      "  Epoch  100: Loss = 29.152679, Bare Loss = 15.451424, Features = 100\n",
      "\tConverged at epoch 136 (tolerance: 1.00e-06)\n",
      "\n",
      "=== Starting Phase 4/6 ===\n",
      "\tAdam (both parameter groups), lambda=3.0512295352156555, nu=0.3\n",
      "  Epoch    0: Loss = 31.126217, Bare Loss = 15.455580, Features = 100\n",
      "  Epoch   50: Loss = 31.320454, Bare Loss = 15.722064, Features = 100\n",
      "\tConverged at epoch 98 (tolerance: 1.00e-06)\n",
      "\n",
      "=== Starting Phase 5/6 ===\n",
      "\tAdam (both parameter groups), lambda=3.299877330333125, nu=0.2\n",
      "  Epoch    0: Loss = 31.769470, Bare Loss = 15.726972, Features = 100\n",
      "  Epoch   50: Loss = 31.944263, Bare Loss = 15.659649, Features = 100\n",
      "  Epoch  100: Loss = 31.595707, Bare Loss = 15.663626, Features = 100\n",
      "\tConverged at epoch 104 (tolerance: 1.00e-06)\n",
      "\n",
      "=== Starting Phase 6/6 ===\n",
      "\tISTA (penalized), lambda=3.4641685485839844, nu=0.1\n",
      "  Epoch    0: Loss = 31.930065, Bare Loss = 15.665719, Features = 5\n",
      "  Epoch   50: Loss = 31.181095, Bare Loss = 15.743423, Features = 5\n",
      "\tConverged at epoch 67 (tolerance: 1.00e-08)\n",
      "  Epoch    0: Loss = 17.424870, Bare Loss = 17.424870\n",
      "  Epoch   50: Loss = 14.569423, Bare Loss = 14.569423\n",
      "  Epoch  100: Loss = 14.340098, Bare Loss = 14.340098\n",
      "  Epoch  150: Loss = 14.259468, Bare Loss = 14.259468\n",
      "  Epoch  200: Loss = 14.256715, Bare Loss = 14.256715\n",
      "Converged after 213 epochs. Relative loss change below 1e-10\n",
      "\n",
      "=== Regression Results ===\n",
      "Selected features: 5 / 100\n",
      "Lambda QUT: 3.4642\n",
      "Train R²: 0.9419\n",
      "Test R²: 0.9669\n",
      "Train MSE: 1.1188\n",
      "Test MSE: 0.7770\n",
      "Selected feature indices: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Fit HarderLASSO Regressor\n",
    "print(\"Training HarderLASSO Regressor...\")\n",
    "\n",
    "model_reg = HarderLASSORegressor(\n",
    "    hidden_dims=(20, ),  # One hidden layers\n",
    "    penalty='harder'\n",
    ")\n",
    "\n",
    "#model_reg.fit(X_train, y_train, verbose=True)\n",
    "model_reg.fit(X, y, verbose=True)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model_reg.predict(X_train)\n",
    "y_pred_test = model_reg.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "train_metrics = model_reg.score(X_train, y_train)\n",
    "test_metrics = model_reg.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n=== Regression Results ===\")\n",
    "print(f\"Selected features: {len(model_reg.selected_features_indices_)} / {n_features}\")\n",
    "print(f\"Lambda QUT: {model_reg.lambda_qut_:.4f}\")\n",
    "print(f\"Train R²: {train_metrics['R2']:.4f}\")\n",
    "print(f\"Test R²: {test_metrics['R2']:.4f}\")\n",
    "print(f\"Train MSE: {train_metrics['MSE']:.4f}\")\n",
    "print(f\"Test MSE: {test_metrics['MSE']:.4f}\")\n",
    "print(f\"Selected feature indices: {model_reg.selected_features_indices_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35def839",
   "metadata": {},
   "source": [
    "### 2.2 Comparison with Different Penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41dfca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different penalty functions...\n",
      "\n",
      "Training with harder penalty...\n",
      "  Test R²: 0.9602\n",
      "  Selected features: 5\n",
      "  Lambda: 3.4387\n",
      "\n",
      "Training with l1 penalty...\n",
      "  Test R²: 0.8675\n",
      "  Selected features: 3\n",
      "  Lambda: 3.4088\n",
      "\n",
      "=== Penalty Comparison ===\n",
      "        test_r2  n_features  lambda\n",
      "harder   0.9602         5.0  3.4387\n",
      "l1       0.8675         3.0  3.4088\n"
     ]
    }
   ],
   "source": [
    "# Compare different penalty functions\n",
    "penalties = ['harder', 'l1']\n",
    "results = {}\n",
    "\n",
    "print(\"Comparing different penalty functions...\")\n",
    "\n",
    "for penalty in penalties:\n",
    "    print(f\"\\nTraining with {penalty} penalty...\")\n",
    "\n",
    "    model = HarderLASSORegressor(\n",
    "        hidden_dims=(20,),\n",
    "        penalty=penalty\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "    # Evaluate\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    n_selected = len(model.selected_features_indices_)\n",
    "\n",
    "    results[penalty] = {\n",
    "        'test_r2': test_score['R2'],\n",
    "        'n_features': n_selected,\n",
    "        'lambda': model.lambda_qut_\n",
    "    }\n",
    "\n",
    "    print(f\"  Test R²: {test_score['R2']:.4f}\")\n",
    "    print(f\"  Selected features: {n_selected}\")\n",
    "    print(f\"  Lambda: {model.lambda_qut_:.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "print(\"\\n=== Penalty Comparison ===\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f71d9",
   "metadata": {},
   "source": [
    "## 3. Classification Examples {#classification}\n",
    "\n",
    "### 3.1 Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3df910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading digits dataset...\n",
      "Dataset shape: (1797, 64)\n",
      "Number of classes: 10\n",
      "Class distribution: [178 182 177 183 181 182 181 179 174 180]\n",
      "\n",
      "Training set: (1437, 64)\n",
      "Test set: (360, 64)\n"
     ]
    }
   ],
   "source": [
    "# Load digits dataset\n",
    "print(\"Loading digits dataset...\")\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "\n",
    "print(f\"Dataset shape: {X_digits.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_digits)}\")\n",
    "\n",
    "# Split the data\n",
    "X_train_dig, X_test_dig, y_train_dig, y_test_dig = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, stratify=y_digits\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_dig.shape}\")\n",
    "print(f\"Test set: {X_test_dig.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e88c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HarderLASSO Classifier...\n",
      "\n",
      "=== Classification Results ===\n",
      "Selected features: 9 / 64\n",
      "Lambda QUT: 169.0077\n",
      "Train accuracy: 0.9576\n",
      "Test accuracy: 0.8833\n",
      "Feature reduction: 85.9%\n"
     ]
    }
   ],
   "source": [
    "# Train HarderLASSO Classifier\n",
    "print(\"Training HarderLASSO Classifier...\")\n",
    "\n",
    "model_clf = HarderLASSOClassifier(\n",
    "    hidden_dims=(20, ),\n",
    "    penalty='harder'\n",
    ")\n",
    "\n",
    "model_clf.fit(X_train_dig, y_train_dig, verbose=False)\n",
    "\n",
    "train_metrics = model_clf.score(X_train_dig, y_train_dig)\n",
    "test_metrics = model_clf.score(X_test_dig, y_test_dig)\n",
    "\n",
    "print(f\"\\n=== Classification Results ===\")\n",
    "print(f\"Selected features: {len(model_clf.selected_features_indices_)} / {X_digits.shape[1]}\")\n",
    "print(f\"Lambda QUT: {model_clf.lambda_qut_:.4f}\")\n",
    "print(f\"Train accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Feature reduction: {(1 - len(model_clf.selected_features_indices_)/X_digits.shape[1]):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4f76a",
   "metadata": {},
   "source": [
    "### 3.2 Breast Cancer Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b43af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading digits dataset...\n",
      "Dataset shape: (569, 30)\n",
      "Number of classes: 2\n",
      "Class distribution: [212 357]\n",
      "\n",
      "Training set: (455, 30)\n",
      "Test set: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load digits dataset\n",
    "print(\"Loading digits dataset...\")\n",
    "X_breast, y_breast = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "print(f\"Dataset shape: {X_breast.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_breast))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_breast)}\")\n",
    "\n",
    "# Split the data\n",
    "X_train_breast, X_test_breast, y_train_breast, y_test_breast = train_test_split(\n",
    "    X_breast, y_breast, test_size=0.2, stratify=y_breast\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_breast.shape}\")\n",
    "print(f\"Test set: {X_test_breast.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae34796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier on breast cancer data...\n",
      "\n",
      "=== Breast Cancer Classification ===\n",
      "Test accuracy: 0.9737\n",
      "Selected features: 3 / 100\n",
      "\n",
      "Selected feature names:\n",
      "  1. feature_20\n",
      "  2. feature_21\n",
      "  3. feature_27\n"
     ]
    }
   ],
   "source": [
    "# Train classifier with feature names\n",
    "print(\"Training classifier on breast cancer data...\")\n",
    "\n",
    "model_cancer = HarderLASSOClassifier(\n",
    "    hidden_dims=(20, 10),\n",
    "    penalty='harder'\n",
    ")\n",
    "\n",
    "model_cancer.fit(X_train_breast, y_train_breast, verbose=False)\n",
    "\n",
    "# Evaluate\n",
    "test_acc_cancer = model_cancer.score(X_test_breast, y_test_breast)['accuracy']\n",
    "selected_features_cancer = model_cancer.selected_features_\n",
    "\n",
    "print(f\"\\n=== Breast Cancer Classification ===\")\n",
    "print(f\"Test accuracy: {test_acc_cancer:.4f}\")\n",
    "print(f\"Selected features: {len(selected_features_cancer)} / {X.shape[1]}\")\n",
    "print(f\"\\nSelected feature names:\")\n",
    "for i, feature in enumerate(selected_features_cancer[:10]):\n",
    "    print(f\"  {i+1}. {feature}\")\n",
    "if len(selected_features_cancer) > 10:\n",
    "    print(f\"  ... and {len(selected_features_cancer) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35a77f",
   "metadata": {},
   "source": [
    "## 4. Survival Analysis Examples {#survival}\n",
    "\n",
    "### 4.1 Rossi Recidivism Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4599e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Rossi recidivism dataset...\n",
      "Dataset shape: (432, 9)\n",
      "Columns: ['week', 'arrest', 'fin', 'age', 'race', 'wexp', 'mar', 'paro', 'prio']\n",
      "\n",
      "First few rows:\n",
      "   week  arrest  fin  age  race  wexp  mar  paro  prio\n",
      "0    20       1    0   27     1     0    0     1     3\n",
      "1    17       1    0   18     1     0    0     1     8\n",
      "2    25       1    0   19     0     1    0     1    13\n",
      "3    52       0    1   23     1     1    1     1     1\n",
      "4    52       0    0   19     0     1    0     1     3\n",
      "\n",
      "Survival data summary:\n",
      "Number of events: 114 / 432 (26.4%)\n",
      "Median follow-up time: 52.0 weeks\n",
      "Features: ['fin', 'age', 'race', 'wexp', 'mar', 'paro', 'prio']\n"
     ]
    }
   ],
   "source": [
    "# Load Rossi dataset\n",
    "print(\"Loading Rossi recidivism dataset...\")\n",
    "df_rossi = load_rossi()\n",
    "\n",
    "print(f\"Dataset shape: {df_rossi.shape}\")\n",
    "print(f\"Columns: {list(df_rossi.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_rossi.head())\n",
    "\n",
    "# Prepare data\n",
    "X_rossi = df_rossi.drop(columns=['week', 'arrest'])\n",
    "time_rossi = df_rossi['week']\n",
    "event_rossi = df_rossi['arrest']\n",
    "\n",
    "print(f\"\\nSurvival data summary:\")\n",
    "print(f\"Number of events: {event_rossi.sum()} / {len(event_rossi)} ({event_rossi.mean():.1%})\")\n",
    "print(f\"Median follow-up time: {time_rossi.median():.1f} weeks\")\n",
    "print(f\"Features: {list(X_rossi.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fab882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HarderLASSO Cox model...\n",
      "\n",
      "=== Cox Regression Results ===\n",
      "Concordance index: 0.6310\n",
      "Selected features: 2 / 7\n",
      "Lambda QUT: 0.5420\n",
      "\n",
      "Selected features:\n",
      "  - age\n",
      "  - prio\n",
      "\n",
      "Coefficients (hazard ratios):\n",
      "  age: -0.369 (HR: 0.691)\n",
      "  prio: 0.312 (HR: 1.366)\n"
     ]
    }
   ],
   "source": [
    "# Train HarderLASSO Cox model\n",
    "print(\"Training HarderLASSO Cox model...\")\n",
    "\n",
    "model_cox = HarderLASSOCox(\n",
    "    hidden_dims=None, # Linear model used\n",
    "    penalty='harder'\n",
    ")\n",
    "\n",
    "model_cox.fit(X_rossi, (time_rossi, event_rossi), verbose=False)\n",
    "\n",
    "# Evaluate model\n",
    "concordance_index = model_cox.score(X_rossi, (time_rossi, event_rossi))['C-index']\n",
    "selected_features_cox = model_cox.selected_features_\n",
    "\n",
    "print(f\"\\n=== Cox Regression Results ===\")\n",
    "print(f\"Concordance index: {concordance_index:.4f}\")\n",
    "print(f\"Selected features: {len(selected_features_cox)} / {X_rossi.shape[1]}\")\n",
    "print(f\"Lambda QUT: {model_cox.lambda_qut_:.4f}\")\n",
    "\n",
    "print(f\"\\nSelected features:\")\n",
    "for feature in selected_features_cox:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Show coefficients for selected features\n",
    "if hasattr(model_cox, 'coef_') and len(selected_features_cox) > 0:\n",
    "    print(f\"\\nCoefficients (hazard ratios):\")\n",
    "    selected_indices = model_cox.selected_features_indices_\n",
    "    coefficients = model_cox.coef_[selected_indices]\n",
    "\n",
    "    for feature, coef in zip(selected_features_cox, coefficients):\n",
    "        hr = np.exp(coef)\n",
    "        print(f\"  {feature}: {coef:.3f} (HR: {hr:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
